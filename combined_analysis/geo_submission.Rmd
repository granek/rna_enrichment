---
title: "Untitled"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(readr)
library(here)
library(dplyr)
library(stringr)
library(tools)
library(tibble)
library(fs)
library(tidyr)
```

```{r}
data_base="/space/hts_for_geo/HTS_course/"
```

```
# pull from DDS
ddsclient download -p HTS_course --include hts_2017_data --include hts_2019_data --include hts_2018_data

# upload to GEO
lftp geoftp@ftp-private.ncbi.nlm.nih.gov/uploads/jagranek_Ef8niGoI
```

# 2019 pilot data
Only dealing with the 2019 libraries that were generated from the 2018 YPD sample RNA. The rest of the 2019 pilot data has already been deposited.
```{r}
pilot2019_data_base = file.path(data_base, "hts_2019_data")
pilot2019_fastq_dir = file.path(pilot2019_data_base, "hts2019_pilot_rawdata")
pilot2019_count_dir = file.path(pilot2019_data_base, "hts2019_pilot_counts")

file.path(pilot2019_fastq_dir, "md5_checksum.txt") %>%
  read_delim(" ", trim_ws = TRUE, col_names = c("md5", "filename")) ->
  pilot2019_fastq_md5sums

file.path(pilot2019_fastq_dir, "2019_pilot_metadata.tsv") %>%
  read_tsv %>%
  filter(sample_year!=2019) ->
  pilot2019_metadata
```

```{r}
list.files(pilot2019_fastq_dir) %>%
  str_subset(".fastq.gz", negate = TRUE)
```

# 2018 data
```{r}
hts2018_data_base = file.path(data_base, "hts_2018_data")
pilot2018_fastq_dir = file.path(hts2018_data_base, "hts2018_pilot_rawdata")
pilot2018_count_dir = file.path(hts2018_data_base, "hts2018_pilot_counts")

file.path(pilot2018_fastq_dir, "md5_checksum.txt") %>%
  read_delim(" ", trim_ws = TRUE, col_names = c("md5", "filename")) ->
  pilot2018_fastq_md5sums

file.path(pilot2018_fastq_dir, "2018_pilot_metadata.tsv") %>%
  read_tsv ->
  pilot2018_metadata
```

```{r}
list.files(pilot2018_fastq_dir) %>%
  str_subset(".fastq.gz", negate = TRUE)
```

## Sanity Check for Duplicated Names
### Check that all FASTQ names are unique (no duplicates between 2018 and 2019)
```{r}
list.files(pilot2018_fastq_dir) %>%
  str_subset(".fastq.gz", negate = FALSE) ->
  pilot2018_fastqs

list.files(pilot2019_fastq_dir) %>%
  str_subset(".fastq.gz", negate = FALSE) ->
  pilot2019_fastqs

duplicated(c(pilot2018_fastqs,pilot2019_fastqs)) %>%
  any
```

### Check that all Count names are unique (no duplicates between 2018 and 2019)
```{r}
duplicated(c(
  list.files(pilot2018_count_dir),
  list.files(pilot2019_count_dir)
)) %>%
  any
```

## Consolidate all 4 lanes of FASTQs 
```{r}
list.files(pilot2018_fastq_dir) %>%
  str_subset(".fastq.gz", negate = FALSE) %>%
  as_tibble %>%
  mutate(stripped_filename=str_remove(value, "_R1_001.fastq.gz")) %>%
  tidyr::separate(stripped_filename, into=c("library_name", "lane"), sep="(_S\\d+_)") %>%
  mutate(lane=paste(lane,"fastq", sep="_")) %>%
  pivot_wider(names_from=lane, values_from=value) ->
  pilot2018_fastq_df
pilot2018_fastq_df
```

# SAMPLES
```{r}
fastq_manifest_path %>%
  read_csv %>%
  filter(group=="pilot") %>%
  transmute(`Sample name`= paste("2017_pilot", seq_along(sample), sep="_"),
         title=str_remove(sample, "_S\\d"),
         `source name`="bacterial culture",
         organism="Pseudomonas syringae",
         `characteristics: strain`="pv. tomato DC3000",
         `characteristics: genotype`="wild type",
         `characteristics: treatment`= case_when(
           media == "mm" ~ "minimal_media",
           media == "kb" ~ "kings_B_media"),
         molecule="total RNA",
         description="",
         `processed data file`= miseq_filename,
         `raw file`= basename(fastq)
  ) ->
  geo_samples

geo_samples
```

# PROTOCOLS	

Protocols applicable to only a subset of Samples can be included as additional columns of the SAMPLES section above instead.	

growth protocol	
treatment protocol 	
extract protocol	
library construction protocol	rRNA was depleted from purified total RNA using the Ribo-Zero Gram-Negative Bacteria kit (Illumina). rRNA-depleted samples were then cleaned up with the RNA Clean & Concentrator-5 (Zymo Research) and eluted in 15ul.  
library strategy	RNA-Seq.  RNA-Seq libraries were prepared using the NEBNext® Ultra™ Directional RNA Library Prep Kit for Illumina® (E7420; New England BioLabs).

	
# DATA PROCESSING PIPELINE	
# Data processing steps include base-calling, alignment, filtering, peak-calling, generation of normalized abundance measurements etc…	
# For each step provide a description, as well as software name, version, parameters, if applicable.	
data processing step	General read quality was evaluated using FastQC v0.11.2
data processing step	Genome and GTF files were downloaded from NCBI and indexed using bowtie2-build (Bowtie2 v2.3.1).
data processing step	FASTQs were filtered and trimmed using fastq-mcf (ea-utils v1.1.2) using paramteres "-q 20 -x 0.5" and adapter sequences from the NEBNext manual.  
data processing step	Filtered and trimmed reads were mapped using tophat2 (v2.1.1) using parameters "--library-type fr-firststrand --max-intron-length 5 --min-intron-length 4 --transcriptome-max-hits 1 --max-multihits 1 --no-coverage-search --no-novel-juncs --no-sort-bam
data processing step	samtools v0.1.19 was used to merge and sort the accepted_hits.bam and unmapped.bam files output by tophat using commands "samtools merge -n" and "samtools sort -n"
data processing step	htseq-count (htseq v0.6.1) was used to count reads per gene with the parameters "--order=name --format=bam --stranded=reverse --type=gene --idattr=locus_tag" $TH_DIR/${SAMPLE}/merged.name.bam
data processing step	A Jupyter notebook containing the analysis pipeline is available at <https://gitlab.oit.duke.edu/Biostatistics_and_Bioinformatics/HTS_SummerCourse_2017/-/blob/master/Materials/Computation/Wk4_Day4_AM/analysis_nb/2017_generate_pilot_counts.ipynb>
genome build	GCF_000007805.1_ASM780v1
processed data files format and content	tab-delimited text files containing raw read counts per gene

## PROCESSED DATA FILES
```{r}
"Data_Info_and_Results/2017_HTS/counts/HTS_2017_pilot" %>%
  here %>%
  list.files(full.names = TRUE) %>%
  md5sum %>%
  enframe(name="full_path", value="md5sum") %>%
  mutate(`file name`=basename(full_path)) %>%
  select(`file name`, `file checksum`=md5sum) ->
  processed_md5

processed_md5
```

```{r}
fastq_manifest_path %>%
  read_csv %>%
  filter(group=="pilot") %>%
  transmute(`file name`= miseq_filename,
         `file type`="raw counts"
         ) %>%
  full_join(processed_md5, by="file name") ->
  geo_processed_data_files

geo_processed_data_files
```

# RAW FILES
```{r}
fastq_manifest_path %>%
  read_csv %>%
  filter(group=="pilot") %>%
  transmute(`file name`= basename(fastq),
         `file type`="fastq",
         `file checksum` = md5,
         `instrument model`="Illumina MiSeq",
         `single or paired-end`="single"
         ) ->
  geo_raw_files

geo_raw_files
```

# Copy Files
```{r eval=FALSE, include=FALSE}
geo_dir="/space/hts_for_geo/2017_files_for_geo"
dir.create(geo_dir)

list.files("/space/hts_for_geo/HTS_course/hts_2017_data/hts2017_pilot_rawdata/", pattern = "Pst-.*_L001_R1_001.fastq.gz", full.names = TRUE) %>%
  file_copy(geo_dir)

list.files(geo_dir, full.names = TRUE)
```

```{r eval=FALSE, include=FALSE}
"Data_Info_and_Results/2017_HTS/counts/HTS_2017_pilot" %>%
  here %>%
  list.files(recursive = TRUE, full.names = TRUE) %>%
  file_copy(geo_dir)

list.files(geo_dir, full.names = TRUE)
```
